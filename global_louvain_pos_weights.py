import networkx as nx
from community_status import Status
import numpy as np
import utils
import time
from measures import modularity
from copy import deepcopy
from measures import NMI


__PASS_MAX = -1
__MIN = 0.0000001


def best_partition(graph, partition=None, weight='weight', resolution=1., randomize=None, random_state=None):
	"""Computes the partition of the graph nodes which maximizes the modularity using the Louvain heuristices
	This is the partition of highest modularity, i.e. the highest partition of the dendrogram generated by the Louvain algorithm.
	"""
	denrdo = generate_dendrogram(graph, partition, weight, resolution, randomize, random_state)    
	return partition_at_level(denrdo, len(denrdo) - 1)


def generate_dendrogram(graph, part_init=None, weight='weight', resolution=1., randomize=None, random_state=None):
	"""Finds communities in the graph and return the associated dendrogram
	A dendrogram is a tree and each level is a partition of the graph nodes.
	Level 0 is the first partition, which contains the smallest communities,
	and the best is len(dendrogram) - 1. The higher the level is, the bigger
	are the communities.
	"""

	random_state = check_random_state(random_state)

	# special case, when there is no link
	# the best partition is everyone in its community
	if graph.number_of_edges() == 0:
		part = dict([])
		for i, node in enumerate(graph.nodes()):
			part[node] = i
		return [part]

	current_graph = graph.copy()
	status = Status()
	status.init(current_graph, weight, part_init)
	status_list = list()

	__one_level(current_graph, status, weight, resolution, random_state)
	# runs the investigation multiple times until the improvement is smaller than the predefined threshold

	new_mod = __modularity(status)
	# computes the modularity of the graph, based on discovered communities

	partition = __renumber(status.node2com)
	# resets the community indices such that they start with 0 and goes up one by one (instead of random list of numbers between 1 and n that we had)

	status_list.append(partition)
	# appends a set of assigned communities at a certain level of detection to $status_list

	mod = new_mod
	# computes the modularity after the first change in community assignment but on the same graph (not induced graph)

	current_graph = induced_graph(partition, current_graph, weight)
	# createsa new graph based on the former one such that each community becomes a node in this new one.

	status.init(current_graph, weight)
	# updates the $status such that obliterates the former one.

	while True:
		__one_level(current_graph, status, weight, resolution, random_state)
		# runs the investigation multiple times until the improvement is smaller than the predefined threshold

		new_mod = __modularity(status)
		if new_mod - mod < __MIN:
		# check if the new modularity after creating a new graph and assign new communities is improved more than threshold in comparison with the former graph or not
		# this if statement checks the modularity value for two different graphs but the stop condition for $__one_level is for one graph with new community assignment
			break
		
		partition = __renumber(status.node2com)
		status_list.append(partition)
		mod = new_mod

		current_graph = induced_graph(partition, current_graph, weight)
		status.init(current_graph, weight)
	# while loop ends when the new induced graph (with more convoluted and merged communities) is no better than the former step.

	return status_list[:]


def check_random_state(seed):
	"""Turn seed into a np.random.RandomState instance.
	Parameters
	----------
	seed : None | int | instance of RandomState
		If seed is None, return the RandomState singleton used by np.random.
		If seed is an int, return a new RandomState instance seeded with seed.
		If seed is already a RandomState instance, return it.
		Otherwise raise ValueError.
	"""
	if seed is None or seed is np.random:
		return np.random.mtrand._rand
	if isinstance(seed, (numbers.Integral, np.integer)):
		return np.random.RandomState(seed)
	if isinstance(seed, np.random.RandomState):
		return seed
	raise ValueError("%r cannot be used to seed a numpy.random.RandomState"
					 " instance" % seed)



def __one_level(graph, status, weight_key, resolution, random_state):
	"""Compute one level of communities
	"""
	modified = True
	nb_pass_done = 0
	cur_mod = __modularity(status)
	new_mod = cur_mod

	while modified and nb_pass_done != __PASS_MAX:
		print('***')

		cur_mod = new_mod
		modified = False
		nb_pass_done += 1
		
		for node in random_state.permutation(list(graph.nodes())):

			com_node = status.node2com[node]

			degc_totw = status.gdegrees.get(node, 0.) / (status.total_weight * 2.)  # NOQA
			# computes the total weight of node by dividing the sum of weights of links incident to the node by (2 * sum of weights of all links in the graph)

			neigh_communities = __neighcom(node, graph, status, weight_key)
			# finds the neighbor nodes and the sum of weights of the shared links between $node and neighbor nodes in each different community

			remove_cost = - resolution * neigh_communities.get(com_node,0) + (status.degrees.get(com_node, 0.) - status.gdegrees.get(node, 0.)) * degc_totw
			# computes how much modularity value will be reduced after 

			__remove(node, com_node, neigh_communities.get(com_node, 0.), status)
			# removes the node from its current community and updates the $status objcet

			best_com = com_node
			best_increase = 0
			for com, dnc in random_state.permutation(list(neigh_communities.items())):
				incr = remove_cost + resolution * dnc - status.degrees.get(com, 0.) * degc_totw
				# computes the delta modularuty value by considering the cost of removing, new gained 
				# modularity (after joining to new community), degree of commmunity, and total weight of ndoe

				if incr > best_increase:
					best_increase = incr
					best_com = com

			__insert(node, best_com, neigh_communities.get(best_com, 0.), status)
			# inserts the node from its current community and updates the $status objcet

			if best_com != com_node:
			# if the community of a node changes
				modified = True

		new_mod = __modularity(status)
		# computes the new modularity of the graph based on new assigned communities

		if new_mod - cur_mod < __MIN:
		# breaks the while loop in the improvement of modularity is smaller than the predefined threshold
			break


def __modularity(status):
	"""
	Fast compute the modularity of the partition of the graph using status precomputed
	"""
	total_weight = float(status.total_weight)
	result = 0.
	for community in set(status.node2com.values()):
	# for all community in the graph
		
		in_degree = status.internals.get(community, 0.)
		# the sum of weights of links inside the community

		degree = status.degrees.get(community, 0.)
		# sum of degrees of nodes inside the community

		if total_weight > 0:
			result += in_degree / total_weight - ((degree / (2. * total_weight)) ** 2)

	return result


def __neighcom(node, graph, status, weight_key):
	"""
	Compute the communities in the neighborhood of node in the graph given
	with the decomposition node2com
	"""

	# keys are the community index of the neighbor nodes
	# values are the sum of the weights of the shared links between $node and the neighbor nodes in each different community

	weights = {}
	for neighbor, datas in graph[node].items():
		if neighbor != node:
			edge_weight = datas.get(weight_key, 1)
			neighborcom = status.node2com[neighbor]
			weights[neighborcom] = weights.get(neighborcom, 0) + edge_weight

	return weights


def __remove(node, com, weight, status):
	""" Remove node from community com and modify status"""
	status.degrees[com] = (status.degrees.get(com, 0.) - status.gdegrees.get(node, 0.))
	# updates the community degree by reducing the degree of node that is going to leave the community

	status.internals[com] = float(status.internals.get(com, 0.) - weight - status.loops.get(node, 0.))
	# updates the community weights by reducing the weight of leaving node (to all nodes in this community) and weight of self loop if there is.

	status.node2com[node] = -1
	# assigns (-1) as the new community of the node that leaved its former community


def __insert(node, com, weight, status):
	""" Insert node into community and modify status"""
	status.node2com[node] = com
	# assigns the new community index of the node

	status.degrees[com] = (status.degrees.get(com, 0.) + status.gdegrees.get(node, 0.))
	# updates the community degree by adding the degree of node that is going to join the community

	status.internals[com] = float(status.internals.get(com, 0.) + weight + status.loops.get(node, 0.))
	# updates the community weights by adding the weight of joining node (to all nodes in this community) and weight of self loop if there is.


def __renumber(dictionary):
	"""Renumber the values of the dictionary from 0 to n
	"""
	count = 0
	ret = dictionary.copy()
	new_values = dict([])

	for key in dictionary.keys():
	# for each node in the graph

		value = dictionary[key]
		# assigned community index of the node

		new_value = new_values.get(value, -1)
		# sets to (-1) for the first time the community index is visited, then catch its value

		if new_value == -1:
		# if this is the first time that community is being visited

			new_values[value] = count
			new_value = count
			count += 1
		# $new_value is the new community index that we use for former community index $value

		ret[key] = new_value
	return ret


def induced_graph(partition, graph, weight="weight"):
	"""Produce the graph where nodes are the communities
	there is a link of weight w between communities if the sum of the weights
	of the links between their elements is w
	"""
	ret = nx.Graph()
	ret.add_nodes_from(partition.values())
	# create one node for each community in the graph in the former level

	for node1, node2, datas in graph.edges(data=True):
		edge_weight = datas.get(weight, 1)
		com1 = partition[node1]
		com2 = partition[node2]
		# finds the edge weight between two nodes and their communities

		w_prec = ret.get_edge_data(com1, com2, {weight: 0}).get(weight, 1)
		ret.add_edge(com1, com2, **{weight: w_prec + edge_weight})
		# creates or updates the link betweek two nodes (or a self link on one node) by assigning a weight to it.

	return ret


def preprocess(graph_base, weight="weight"):
	nodes = list(graph_base.nodes())
	nb_vertices = len(nodes)
	mutuals, max_mutuals, total_mutuals = get_mutuals(graph_base)

	for i in range(nb_vertices):
		neighbors = list(graph_base.neighbors(nodes[i]))
		max_mutual_node = max_mutuals.get(nodes[i])

		for neigh in neighbors:
			if graph_base[nodes[i]][neigh].get('done', False):
				continue
			max_mutual_neigh = max_mutuals.get(neigh)
			w = mutuals.get(nodes[i]).get(neigh)
			w1 = 0.0
			w2 = 0.0
			try:
				w1 = w / max_mutual_node
			except:
				pass
			try:
				w2 = w / max_mutual_neigh
			except:
				pass
			w = (w1 + w2) / 2
			graph_base.add_edge(nodes[i], neigh, weight=w, done=True)


def get_mutuals(graph_base):
	mutuals = {}		# mutual[n1][n2]	= number of mutual neighbors between nodes n1 and n2
	max_mutuals = {}	# max_mutuals[n]	= maximum number of mutuals between node n and any other node = max(mutuals[n][k])
	total_mutuals = {}	# total_mutuals[n]	= number of total mutual of node n with any other node = signma(mutuals[n][k])
	nodes = list(graph_base.nodes())
	nb_vertices = len(nodes)

	# initializing the return values
	for i in range(nb_vertices):
		mutuals[nodes[i]] = {}
		max_mutuals[nodes[i]] = -1
		total_mutuals[nodes[i]] = 0.0

	for i in range(nb_vertices):
		neighbors = list(graph_base.neighbors(nodes[i]))
		for neigh in neighbors:
			if neigh in mutuals[nodes[i]]:
				continue
			cur_mutual = shared_neighbors_cnt(graph_base, nodes[i], neigh)
			mutuals[nodes[i]][neigh] = cur_mutual
			mutuals[neigh][nodes[i]] = cur_mutual

			total_mutuals[nodes[i]] = total_mutuals[nodes[i]] + cur_mutual
			total_mutuals[neigh] = total_mutuals[neigh] + cur_mutual

			if cur_mutual > max_mutuals[nodes[i]]:
				max_mutuals[nodes[i]] = cur_mutual
			if cur_mutual > max_mutuals[neigh]:
				max_mutuals[neigh] = cur_mutual

		# to avoid the double-counting
		total_mutuals[nodes[i]] = total_mutuals[nodes[i]]/2.0

	return mutuals, max_mutuals, total_mutuals


def shared_neighbors_cnt(graph_base, u, v):
	shared = 0
	if(graph_base.degree(u) > graph_base.degree(v)):
		tmp = u
		u = v
		v = tmp
	neighbors_u = graph_base[u]
	neighbors_v = graph_base[v]
	for n1 in neighbors_u:
		if n1 in neighbors_v:
			shared = shared+1
	return shared



def partition_at_level(dendrogram, level):
	"""Return the partition of the nodes at the given level
	A dendrogram is a tree and each level is a partition of the graph nodes.
	Level 0 is the first partition, which contains the smallest communities,
	and the best is len(dendrogram) - 1.
	The higher the level is, the bigger are the communities
	"""
	partition = dendrogram[0].copy()
	for index in range(1, level + 1):
		for node, community in partition.items():
			partition[node] = dendrogram[index][community]
	return partition


def main():
	start_time = time.time()

	args = utils.create_argument_parser()
	graph = utils.load_graph(args.dataset, args.w)
	graph_copy = deepcopy(graph)

	preprocess(graph)
	partition = best_partition(graph)

	finish_time = time.time()
	print('\nDone in %.4f seconds.' %(finish_time - start_time))

	communities = utils.extract_communities(partition)
	utils.print_comm_info_to_display(communities)
	# utils.write_comm_info_to_file(partition)

	print('modularity_value =', modularity(graph_copy, communities))
	print('NMI =', NMI(args.output, partition))

	finish_time = time.time()
	print('\nDone in %.4f seconds.' %(finish_time - start_time))


if __name__ == "__main__":
	main()